{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceb5041c",
   "metadata": {},
   "source": [
    "# Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008458cf",
   "metadata": {},
   "source": [
    "Min-Max scaling, also known as Min-Max normalization, is a data preprocessing technique used to transform features in a dataset to a specific range, typically between 0 and 1. This scaling method is particularly useful when the features in your dataset have different scales, and you want to ensure that they all have a similar range for modeling purposes. Min-Max scaling linearly transforms the original values to the new range while preserving the relative relationships between the data points.\n",
    "\n",
    "The formula for Min-Max scaling is as follows for a single feature:\n",
    "\n",
    "X _scaled = (X − X_min)/(X_max - X_min)\n",
    "\n",
    "Where:\n",
    "X scaled is the scaled value of the feature \n",
    "X is the original value of the feature.\n",
    "X min is the minimum value of the feature in the dataset.\n",
    "X maxis the maximum value of the feature in the dataset.\n",
    "\n",
    "Here's an example to illustrate Min-Max scaling:\n",
    "\n",
    "Suppose you have a dataset of exam scores with the following values for a specific test:\n",
    "\n",
    "Student A: 65\n",
    "Student B: 78\n",
    "Student C: 90\n",
    "Student D: 50\n",
    "\n",
    "To apply Min-Max scaling to these scores, you would first calculate the minimum and maximum values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31cf6f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "data = {'Student_A': [65], 'Student_B': [78], 'Student_C': [90], 'Student_D': [50]}\n",
    "df = pd.DataFrame(data)\n",
    "min_max = MinMaxScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a97944fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Student_A</th>\n",
       "      <th>Student_B</th>\n",
       "      <th>Student_C</th>\n",
       "      <th>Student_D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65</td>\n",
       "      <td>78</td>\n",
       "      <td>90</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Student_A  Student_B  Student_C  Student_D\n",
       "0         65         78         90         50"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "981a33aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Student_A', 'Student_B', 'Student_C', 'Student_D'], dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dae04a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MinMaxScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max.fit(df[['Student_A', 'Student_B', 'Student_C', 'Student_D']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fbd14c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max.transform(df[['Student_A', 'Student_B', 'Student_C', 'Student_D']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c9b3d3",
   "metadata": {},
   "source": [
    "# Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e5e6c5",
   "metadata": {},
   "source": [
    "The Unit Vector technique in feature scaling, also known as \"vector normalization\" or \"unit normalization,\" is a method used to scale features in a dataset such that their magnitude or length becomes 1 while preserving their direction. This technique is commonly used in machine learning, particularly in algorithms that rely on distance metrics, such as k-nearest neighbors (KNN) and support vector machines (SVM).\n",
    "\n",
    "Unit Vector scaling is performed as follows for a single feature:\n",
    "\n",
    "X_unit = X/ ∥X∥\n",
    "\n",
    "Where:\n",
    "X_unit is the unit-scaled value of the feature \n",
    "X is the original value of the feature.\n",
    "∥X∥ represents the magnitude or length of the feature vector X, which is calculated as the square root of the sum of squares of its components.\n",
    "\n",
    "\n",
    "The key difference between Unit Vector scaling and Min-Max scaling is that Unit Vector scaling does not constrain the values to a specific range (e.g., 0 to 1) but rather ensures that the magnitude of the feature vector is 1. This can be useful when the direction or relative relationships between feature vectors are more important than their absolute values.\n",
    "\n",
    "Here's an example to illustrate Unit Vector scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "862864a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Points:\n",
      "[[3 4 5]\n",
      " [1 2 2]\n",
      " [4 4 4]\n",
      " [2 1 3]]\n",
      "\n",
      "Unit-Scaled Data Points:\n",
      "[[0.42426407 0.56568542 0.70710678]\n",
      " [0.33333333 0.66666667 0.66666667]\n",
      " [0.57735027 0.57735027 0.57735027]\n",
      " [0.53452248 0.26726124 0.80178373]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the original data points as rows in a NumPy array\n",
    "data_points = np.array([[3, 4, 5],\n",
    "                        [1, 2, 2],\n",
    "                        [4, 4, 4],\n",
    "                        [2, 1, 3]])\n",
    "\n",
    "# Calculate the magnitude (length) of each data point\n",
    "magnitudes = np.linalg.norm(data_points, axis=1)\n",
    "\n",
    "# Perform Unit Vector scaling by dividing each data point by its magnitude\n",
    "unit_scaled_data = data_points / magnitudes[:, np.newaxis]\n",
    "\n",
    "print(\"Original Data Points:\")\n",
    "print(data_points)\n",
    "\n",
    "print(\"\\nUnit-Scaled Data Points:\")\n",
    "print(unit_scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1ed3e2",
   "metadata": {},
   "source": [
    "# Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be737e98",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a dimensionality reduction technique used in statistics and machine learning to transform high-dimensional data into a lower-dimensional representation while preserving as much of the original variance as possible. PCA accomplishes this by finding a set of orthogonal axes, called principal components, along which the data varies the most. These principal components capture the most significant patterns or directions in the data, allowing you to reduce the dimensionality by retaining only the most informative components.\n",
    "\n",
    "Here's an overview of how PCA works:\n",
    "\n",
    "Standardization: PCA often starts with standardizing the features (subtracting the mean and dividing by the standard deviation) to ensure that all features have the same scale. This step is important because PCA is sensitive to the scale of the data.\n",
    "\n",
    "Covariance Matrix: PCA computes the covariance matrix of the standardized data. The covariance matrix quantifies the relationships between pairs of features and helps identify the directions in which the data varies the most.\n",
    "\n",
    "Eigendecomposition: PCA then performs eigendecomposition or singular value decomposition (SVD) on the covariance matrix to obtain the eigenvalues and eigenvectors.\n",
    "\n",
    "Selecting Principal Components: The eigenvalues represent the variance explained by each principal component. Typically, you sort the eigenvalues in descending order and select the top k eigenvectors (principal components) that correspond to the highest eigenvalues. These k principal components capture most of the variance in the data.\n",
    "\n",
    "Projecting Data: Finally, you project the original data onto the selected principal components to obtain the lower-dimensional representation of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3c2f92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Shape: (100, 3)\n",
      "Reduced Data Shape: (100, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Generate a random dataset with 3 features and 100 data points\n",
    "np.random.seed(0)\n",
    "data = np.random.randn(100, 3)\n",
    "\n",
    "# Create a PCA object with 2 components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit the PCA model to the data and transform the data\n",
    "data_reduced = pca.fit_transform(data)\n",
    "\n",
    "# Print the original data shape and reduced data shape\n",
    "print(\"Original Data Shape:\", data.shape)\n",
    "print(\"Reduced Data Shape:\", data_reduced.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf47124",
   "metadata": {},
   "source": [
    "# Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca928c2",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) and feature extraction are closely related concepts in machine learning and data analysis. PCA can be used as a feature extraction technique to reduce the dimensionality of a dataset while retaining the most important information in the original features. Here's a breakdown of the relationship between PCA and feature extraction, along with an example to illustrate how PCA can be used for feature extraction:\n",
    "\n",
    "1. Dimensionality Reduction: Both PCA and feature extraction aim to reduce the dimensionality of a dataset. High-dimensional datasets often suffer from the curse of dimensionality, which can lead to increased computational complexity and the risk of overfitting when training machine learning models.\n",
    "\n",
    "2. Preserving Information: Feature extraction methods, including PCA, aim to retain as much valuable information as possible while reducing dimensionality. In PCA, this is achieved by capturing the variance in the data along the principal components.\n",
    "\n",
    "3. Principal Components as New Features: In PCA, the principal components are linear combinations of the original features. These principal components can be thought of as new features that are derived from the original features. Each principal component represents a direction in the original feature space along which the data varies the most.\n",
    "\n",
    "4. Ordering of Principal Components: Principal components are ordered by the amount of variance they explain. The first principal component explains the most variance, the second explains the second most, and so on. By selecting a subset of the top principal components, you effectively choose a reduced set of features.\n",
    "\n",
    "5. Dimensionality Control: PCA allows you to control the level of dimensionality reduction by specifying the number of principal components to retain. You can choose to retain only a few principal components or a larger number depending on the desired trade-off between dimensionality reduction and information preservation.\n",
    "\n",
    "Here's an example of using PCA for feature extraction in Python:\n",
    "\n",
    "For example, if you have a dataset with 100 features, you can apply PCA to reduce it to, say, 10 principal components. These 10 principal components can then be used as the reduced feature set for further analysis or modeling. This not only reduces computational complexity but can also help mitigate issues related to overfitting in machine learning models.\n",
    "\n",
    "The key idea is that PCA identifies the underlying structure and patterns in the data and represents them with a reduced set of features, allowing you to work with a more manageable and informative representation of your data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019fd511",
   "metadata": {},
   "source": [
    "# Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc45fd50",
   "metadata": {},
   "source": [
    "To preprocess the data for building a recommendation system for a food delivery service, you can use Min-Max scaling to ensure that the features, such as price, rating, and delivery time, are on the same scale. Min-Max scaling will transform these features into a common range (typically 0 to 1) while preserving their relative relationships. Here's how you can use Min-Max scaling step by step:\n",
    "\n",
    "Understand the Data: Begin by understanding the characteristics of your dataset, including the range and distribution of the features. In your case, you have features like price, rating, and delivery time.\n",
    "\n",
    "Identify the Range: Determine the minimum and maximum values for each feature. For example:\n",
    "\n",
    "Minimum Price: $5.00\n",
    "Maximum Price: $30.00\n",
    "Minimum Rating: 2.0\n",
    "Maximum Rating: 5.0\n",
    "Minimum Delivery Time (in minutes): 15\n",
    "Maximum Delivery Time (in minutes): 60\n",
    "Apply Min-Max Scaling: For each feature, use the Min-Max scaling formula to scale the values to the range [0, 1]:\n",
    "\n",
    "For price:\n",
    "Scaled Price = (Price−Minimum Price) / (Maximum Price − Minimum Price)\n",
    "\n",
    "For rating :\n",
    "Scaled Rating = (Rating−Minimum Rating) / (Maximum Rating − Minimum Rating)\n",
    "\n",
    "For delivery time:\n",
    "Scaled Delivery Time = (Delivery Time - Minimum Delivery Time) / (Maximum Delivery Time - Minimum Delivery Time)\n",
    "\n",
    "\n",
    "\n",
    "Perform Scaling: Apply the scaling transformations to all the data points in your dataset for each respective feature. This will ensure that all values fall within the [0, 1] range.\n",
    "\n",
    "Updated Dataset: Your preprocessed dataset will now have scaled values for price, rating, and delivery time. These scaled features can be used as input for building your recommendation system.\n",
    "\n",
    "Normalization Parameters: Keep track of the minimum and maximum values for each feature since you'll need these parameters to reverse the scaling when making recommendations to users. You'll need to map scaled values back to their original ranges to provide meaningful recommendations.\n",
    "\n",
    "Min-Max scaling is particularly useful in cases where you want to ensure that all features contribute equally to your recommendation system and where the absolute values of the features may vary widely. Once the data is scaled, you can apply various recommendation algorithms, such as collaborative filtering or content-based filtering, to make personalized food recommendations to users based on their preferences and needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d87e98",
   "metadata": {},
   "source": [
    "# Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48675163",
   "metadata": {},
   "source": [
    "Using Principal Component Analysis (PCA) to reduce the dimensionality of a dataset for predicting stock prices can be a valuable approach, especially when dealing with a dataset that contains many features. Here's how you can use PCA for dimensionality reduction in the context of building a stock price prediction model:\n",
    "\n",
    "Data Preparation:\n",
    "\n",
    "Gather your dataset, which includes various features related to company financial data and market trends. Ensure that the data is cleaned and preprocessed, handling missing values and outliers appropriately.\n",
    "\n",
    "Standardization:\n",
    "\n",
    "It's important to standardize your features (subtract the mean and divide by the standard deviation) before applying PCA. Standardization ensures that all features have the same scale, which is a prerequisite for PCA to work effectively. The reason is that PCA is sensitive to the scale of the data.\n",
    "\n",
    "Choosing the Number of Principal Components:\n",
    "\n",
    "Decide on the number of principal components (PCs) to retain in your reduced feature set. You can choose based on a desired explained variance threshold or by considering the trade-off between dimensionality reduction and information loss. A common approach is to start with a relatively small number of PCs and gradually increase it while monitoring how much variance they explain.\n",
    "\n",
    "Applying PCA:\n",
    "\n",
    "Use PCA to calculate the principal components. This can be done using libraries such as scikit-learn in Python. Fit the PCA model to your standardized dataset.\n",
    "\n",
    "Explained Variance:\n",
    "\n",
    "After fitting the PCA model, you can examine the explained variance for each principal component. The explained variance tells you how much of the total variance in the data is captured by each PC. This information can help you decide on the number of PCs to retain.\n",
    "\n",
    "Selecting Principal Components:\n",
    "\n",
    "Based on your criteria (e.g., a specific explained variance threshold), select the appropriate number of principal components that you want to keep.\n",
    "\n",
    "Transforming the Data:\n",
    "\n",
    "Use the selected principal components to transform your original dataset into a lower-dimensional representation. This reduced dataset contains the principal components as new features.\n",
    "\n",
    "Model Building:\n",
    "\n",
    "Use the reduced dataset (with the selected principal components) as input for your stock price prediction model. You can apply various machine learning algorithms, such as regression models or time series forecasting techniques, to build your predictive model.\n",
    "\n",
    "By applying PCA and reducing the dimensionality of your dataset, you can potentially improve the efficiency of your model training and reduce the risk of overfitting. It also helps in identifying the most influential features (principal components) in explaining the variance in stock price movements, which can be valuable for your prediction task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8ed7fb",
   "metadata": {},
   "source": [
    "# Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6cfd81ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = [1, 5, 10, 15, 20]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df = pd.DataFrame(data, columns=['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2e84968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   value\n",
       "0      1\n",
       "1      5\n",
       "2     10\n",
       "3     15\n",
       "4     20"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e176997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b41f241",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc3b299e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MinMaxScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max.fit(df[['value']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "88ae360b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.21052632],\n",
       "       [0.47368421],\n",
       "       [0.73684211],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max.transform(df[['value']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3c3d7f",
   "metadata": {},
   "source": [
    "# Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "540a63eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio:\n",
      "[8.16725164e-01 1.28846359e-01 4.43226017e-02 1.01058752e-02\n",
      " 3.60739237e-37]\n",
      "\n",
      "Cumulative Variance Explained:\n",
      "[0.81672516 0.94557152 0.98989412 1.         1.        ]\n",
      "\n",
      "Number of Components to Retain 95% Variance: 3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "# Create a sample dataset with features: height, weight, age, gender, blood pressure\n",
    "data = {\n",
    "    'height': [165, 170, 175, 160, 180],\n",
    "    'weight': [60, 70, 75, 55, 90],\n",
    "    'age': [30, 25, 35, 40, 28],\n",
    "    'gender': [0, 1, 1, 0, 1],  # Assuming 0 for male, 1 for female\n",
    "    'blood_pressure': [120, 130, 125, 115, 140]\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Standardize the features (mean = 0, standard deviation = 1)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "standardized_data = scaler.fit_transform(df)\n",
    "\n",
    "# Create a PCA object\n",
    "pca = PCA()\n",
    "\n",
    "# Fit the PCA model to the standardized data\n",
    "pca.fit(standardized_data)\n",
    "\n",
    "# Calculate the explained variance ratio for each principal component\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Calculate the cumulative explained variance\n",
    "cumulative_variance_explained = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Find the number of components that explain at least 95% of the variance\n",
    "n_components_95 = np.argmax(cumulative_variance_explained >= 0.95) + 1\n",
    "\n",
    "print(\"Explained Variance Ratio:\")\n",
    "print(explained_variance_ratio)\n",
    "print(\"\\nCumulative Variance Explained:\")\n",
    "print(cumulative_variance_explained)\n",
    "print(\"\\nNumber of Components to Retain 95% Variance:\", n_components_95)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02a21da",
   "metadata": {},
   "source": [
    "We create a sample dataset with five features: height, weight, age, gender, and blood pressure.\n",
    "\n",
    "We standardize the features using StandardScaler to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "We create a PCA object and fit it to the standardized data.\n",
    "\n",
    "We calculate the explained variance ratio for each principal component and the cumulative explained variance.\n",
    "\n",
    "We find the number of components needed to retain at least 95% of the variance and print the results.\n",
    "\n",
    "The n_components_95 variable will give you the number of principal components you would choose to retain based on preserving 95% of the variance in the data. You can adjust the variance threshold according to your specific requirements.\n",
    "\n",
    "Decide on the number of principal components to retain based on your goals and the amount of variance you want to preserve. Common choices include:\n",
    "\n",
    "Retain enough components to capture a certain percentage of the total variance (e.g., 95%).\n",
    "Choose a number of components that explain a significant portion of the variance while reducing dimensionality.\n",
    "The decision should balance between dimensionality reduction and retaining enough information for your analysis or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d271ff67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
